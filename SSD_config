# SSD with Mobilenet v1, configured for Oxford-IIIT Pets Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for "gs://fingerreader_od/data" to find the fields that
# should be configured.

model {
  ssd {
    num_classes: 13
    box_coder {
      faster_rcnn_box_coder {  #these are scale factors to creat the loss in ssd. we scale the t values in the loss (there are 4 regression values)
        y_scale: 10.0                              #with these factors. Good to keep this defaults 
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {  #this will match ground truth boxes with the default boxes , making xij value in the paper 
      argmax_matcher {              #matcher object to matching the predicted boxes to real boxes 
        matched_threshold: 0.   #matched treshold  if our score or whatever is more than 0.5 we take it as a similar one 
        unmatched_threshold: 0.5  #unmatched    There is no medium similarity of two boxes or what ever , this is a treshold 
        ignore_thresholds: false   #we are not ignoring tresholds 
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {               #we use intersection overunion   not much to do with us 
      }
    }
    anchor_generator {                   #anchor generator parameters 
      ssd_anchor_generator {
        num_layers: 6     #how many featuremaps we use to predict
        min_scale: 0.2  #this is corresponding to Sk in the paper which will decide the scale of the each anchor or default boxes 
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {                 #this is unitialte when the model is bulding very usefull 
      fixed_shape_resizer {
        height: 300              #we can also use 512 but it will make this too slow 
        width: 300
      }
    }
    box_predictor {                       #the box predictor  (This will be importaning in the vanishing grad thing ssd-mobilenet )
      convolutional_box_predictor {
        min_depth: 0         #If max_depth is set to 0, no additional feature map will be inserted before location and class predictions
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false         #do we use convolution on the ssd convolutional box predictor ? 
        dropout_keep_probability: 0.5
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false   we are not making the score between 0 and one 
        conv_hyperparams {    #this is the convolutional kernal hyper parameters which predict boxes good for our issue 
          activation: RELU_6,     ##can we change this ?  what are the activations of the convolutional kernal #this is just if we use addiyional ##Convolutional layer. Because anyways predictor do not use any relu activation . It like the final softmax layer 
          regularizer {
            l2_regularizer {         #this things shouldn't change 
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {        #how to initialize the  convolutional kernal 
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {               #we use batch norm.  do we use batch norm on the kernal 
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }           #This is very useful and very strong set of params . 
      }
    }
    feature_extractor {             #This is always good to keep without changing much .  
      type: 'ssd_mobilenet_v1'  #check the meta acrchitecture 
      min_depth: 16   #Minimum depth value (number of channels) for all convolution ops. #This is useful in creatin additinal feature maps 
      depth_multiplier: 1.0      #This means how much depth we use in separable convolution. IF this is less that one it means the network become
#much lighter    #reduce the number of parameters  #specially whwn creatign new things
      conv_hyperparams {
        activation: RELU_6,        #This is what the relu units have used . This is to apply for conv2D
        regularizer {
          l2_regularizer {
            weight: 0.00004       
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {                     #this is for calcuating the loss 
      classification_loss {
        weighted_sigmoid {             #we take weighted sigmoid loss 
          anchorwise_output: true #may be we can cjange tihs  | 'weighted_softmax'
        }
      }
      localization_loss {
        weighted_smooth_l1 {          #there can be outliers so always good to use l1 loss . This one is more robust 
          anchorwise_output: true
        }



#Here what is happening is first we select the regions with highest losses . Then we take another subset of regions which are more alone in the #sense of IOU(This is again for selecting the hardest examples because of there's some intersection means somehow localization this has done #properly but no intersection means there's a wrong thing ). This is good to avoid false postive alos . 





      }
      hard_example_miner {        #we have a hard example miner .
        num_hard_examples: 3000   #if we use none we take all the examples 
        
#here threshlod means the difference iou between selected hard examples this is more like filering 
        iou_threshold: 0.99    #we only take negatie samples by the loss as in the ssd paper . OF this is something liek 0.7 we do some IOU filteri
        loss_type: CLASSIFICATION            #loss_type ?  this is classificaiton   #can use BOTH 
        max_negatives_per_positive: 3  #from the original paper it is 3 . It means for 1 postive box there should be 3 negative boxes  
        min_negatives_per_image: 0  #minimum number of negative anchors for a given image. Allow sampling negatives in image without any positive #anchors.  So in the hard selected hard losses if there's no positve thing with this we can do minimim number of 
      }
      classification_weight: 1.0            #Do not change . This isn't very mathemtical. Dafault values in SSD paper 
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true  #we devide by the matching anchors as in the paper 


#From 148 - 156  We use parameters to play around output we get in the INFERENCE 



    post_processing {  # Has an effect on effiecny of the inference .
      batch_non_max_suppression {               #this is use to build non max supresser 
        score_threshold: 1e-8   #low scoring boxes are removed  , This is like a score confident to each bounding box 
        iou_threshold: 0.6      #This is to remove the duplicate boudning boxes in side the NMS 
        max_detections_per_class: 100  # Per a class how many detections we take acoding to the probability score 
        max_total_detections: 100 #how many we keep as detectors 
      }
      score_converter: SIGMOID  #we use sigmoid score for each preduicted class in a bounding box 
    }
  }
}



#For optimizers best -  http://www.erogol.com/comparison-sgd-vs-momentum-vs-rmsprop-vs-momentumrmsprop/

train_config: {
  batch_size: 24        #Can change to stability 
  optimizer {
    rms_prop_optimizer: {              #can use adam_optimizer 
      learning_rate: {    #This also has different setting in the optimizer_builder.py 
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004        ##The original paper has used 0.004 so it is always advisable to use a low learning rate as ##possible .  Here we are tuning all the things at one time 
          decay_steps: 800720   #This will effect to the decay rate with (Global_step/Decat steps)
          decay_factor: 0.95
        }
      }        #Here the momentumn is the how much we keep cache 
      momentum_optimizer_value: 0.9      #This just using the momentumn 0.9 is the mu value , Here we can use adam 
      decay: 0.9          
      epsilon: 1.0        #This is for the adagrad rms propwith momentumn 
    }
  }
  fine_tune_checkpoint: "gs://fingerreader_od/data/model.ckpt"
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: "gs://fingerreader_od/data/pet_train.record"
  }
  label_map_path: "gs://fingerreader_od/data/pet_label_map.pbtxt"
}

eval_config: {
  num_examples: 2000
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: "gs://fingerreader_od/data/pet_val.record"
  }
  label_map_path: "gs://fingerreader_od/data/pet_label_map.pbtxt"
  shuffle: false
  num_readers: 1
}
